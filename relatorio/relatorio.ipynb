{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coletor RI\n",
    "\n",
    "**Alunos:**\n",
    "1. Marcelo Ricoy\n",
    "2. Yan Sirianni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Principais desafios, decisões e arquitetura utilizada:\n",
    "\n",
    "**Principais desafios:**\n",
    "- Uso do python (problemas de tipagem, acesso e manipulação do orderedDict, funções que são prontas do proprio python)\n",
    "- primeira interação com um coletor de páginas (dificuldades em entender o funcionamento e implementação do codigo do coletor)\n",
    "\n",
    "**Decisões e Arquitetura:**\n",
    "\n",
    "- Profundidade limite: 3\n",
    "- Número de páginas coletadas: 50000\n",
    "\n",
    "O coletor foi dividido em 3 partes:\n",
    "\n",
    "- Domain: armazena informações importantes sobre o servidor a serem usadas no momento do escalonamento das URLs.\n",
    "- Page Fetcher: serão *threads* responsáveis por fazer as requisições das URLs obtidas por meio do escalonador (instancia da classe `Scheduler`)\n",
    "- Scheduler: é responsável por armazenar as filas de URLs a serem requisitadas\n",
    "\n",
    "\n",
    "\n",
    "b) Foram utilizadas as seguintes URLs sementes no coletor:\n",
    "\n",
    "- https://www.uol.com.br/\n",
    "- https://www.terra.com.br/\n",
    "- https://ge.globo.com/\n",
    "- https://www.em.com.br/\n",
    "- https://www.otempo.com.br/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Como foi feito, faça referências à classes e métodos do código fonte:\n",
    "\n",
    "→ Os critérios de exclusão de robôs e quantidade de tempo entre requisições à um mesmo servidor\n",
    "* A classe domain foi utilizada para registrar o acesso aos dominios e verificar sua acessibilidade levando em conta o tempo limite entre as requisições\n",
    "* A classe scheduler utiliza a função add_new_page adicionar uma nova pagina, se for possivel, para a Fila de URLs por domínio desde as semestes para iniciar quanto as proximas paginas encontrada, após adicionadas as sementes o metodo get_next_url é utilizado para encontrar as proximas urls e nela utilizamos o tempo de 1 segundo entre as requisições à um mesmo servidor.\n",
    "* Na função can_fetch_page verificamos por meio do robots.txt se uma determinada URL pode ser coletada,levando em consideração a política de exclusão de robôs\n",
    "* A classe Page Fecher utiliza o metodo run para coletar enquanto houver páginas a serem coletada, o metodo crawl_new_url para coletar uma nova URL, obtendo-a do escalonador e o metodo request Url no qual usa um request.get para pegar o conteudo da página\n",
    "* O grupo optou por colocar o contador de páginas coletadas dentro do método *request_url* do Page Fetcher\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) O impacto na velocidade de coleta (quantidade de páginas por segundo) ao aumentar o número de threads 1 a 20 de 5 em 5 passos e, logo após, de 30 a 100 com 20 passos . Ao fazer o estudo, colete menos páginas (~100 páginas visitadas pode ser o suficiente).\n",
    "\n",
    "Utilizando de 1 a 20 threads de 5 em 5\n",
    "* 1 -> 477.8 segundos\n",
    "* 5 -> 121.2 segundos\n",
    "* 10 -> 81.7 segundos\n",
    "* 15 -> 74.1 segundos\n",
    "* 20 -> 64.5 segundos\n",
    "\n",
    "Utilizando de 30 a 100 threads de 20 em 20\n",
    "* 30 -> 61.8 segundos\n",
    "* 50 -> 67.0 segundos\n",
    "* 70 -> 67.9 segundos\n",
    "* 90 -> 73.9 segundos\n",
    "* 100 -> 80.7 segundos\n",
    "\n",
    "O maior impacto ao aumentar as threads foi quando passou de 1 para 5, uma melhora de quase 4 vezes mais rapido, após esse valor de threads é possivel perceber que o tempo vai diminuindo conforme o numero de threads vai aumentando até um certo limite onde o aumenta de threads piora a velocidade da coleta. Outro ponto a ser levantado nesse estudo é que com o aumento do numero de threads se torna difícil controlar o limite de paginas visitadas no total, como nessa implementação não a condição de corrida, as threads até receberem que o valor limite foi atingido acabam extrapolando esse limite.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Link para a página descrevendo o coletor criado\n",
    "\n",
    "- https://yansirianni.github.io/ri-crawler/\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
